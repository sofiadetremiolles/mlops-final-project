#!python3
# This first line with #! ("shebang") tells the system that this is a script
# Note that this file has been marked as "executable" for the OS using the command "chmod +x"
import argparse
from html import parser
import logging
import os
import sys
from typing import Any
from pathlib import Path

from dsba.data_ingestion import load_csv_from_path, write_csv_to_path
from dsba.model_registry import list_models_ids, load_model, load_model_metadata
from dsba.model_prediction import classify_dataframe
from dsba.experiment import run_experiment
from dsba.benchmark import report_benchmark

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S,",
)


def create_parser():
    """
    We use the library argparse to define the functionalities of our CLI,
    it will also do the magic to turn the command line typed by the user into an easy to use python object.

    We want to define a command line that does
    dsba_cli [command] [options]
    """
    parser = argparse.ArgumentParser(description="DSBA Plaftorm CLI Tool")
    subparsers = parser.add_subparsers(dest="command", required=True)

    # List models command (it has no additional parameters)
    subparsers.add_parser("list", help="List available models")

    # Predict command (it requires a model name, an input file and an output file)
    predict_parser = subparsers.add_parser("predict", help="Make predictions")
    predict_parser.add_argument("--model", help="Model name to use", required=True)
    predict_parser.add_argument("--input", help="Input file path", required=True)
    predict_parser.add_argument("--output", help="Output file path", required=True)

    # Run command (it requires an input url, a target column and a model name)
    run_experiment_parser = subparsers.add_parser("run", help="Run experiment")
    run_experiment_parser.add_argument("--input", help="Input URL for data", required=True)
    run_experiment_parser.add_argument("--target", help="Target column name in data", required=True)
    run_experiment_parser.add_argument("--model", help="Model name to use", required=True)

    # Report command (it requires a performance metric name)
    report_parser = subparsers.add_parser("report", help="Report benchmark")
    report_parser.add_argument("--metric", help="Performance metric to benchmark on")

    return parser


def get_script_args():
    parser = create_parser()
    return parser.parse_args()


def main():
    args = get_script_args()
    if args.command == "list":
        list_models()
    elif args.command == "predict":
        predict(args.model, args.input, args.output)
    elif args.command == "run":
        run(args.input, args.target, args.model)
    elif args.command == "report":
        report(args.metric)


# We create a few light wrappers around our platform functionalities, just collect inputs and print the results.

def list_models() -> None:
    models = list_models_ids()
    print("Available models:")
    for model in models:
        print(f"- {model}")


def predict(model_id: str, input_file: str, output_file: str) -> None:
    model = load_model(model_id)
    metadata = load_model_metadata(model_id)
    df = load_csv_from_path(input_file)
    predictions = classify_dataframe(model, df, metadata.target_column)
    write_csv_to_path(predictions, output_file)
    print(f"Scored {len(predictions)} records")


def run(input_url: str, target_column: str, model_id: str) -> None:
    metadata = run_experiment(input_url, target_column, model_id)
    print("Performance Metrics:")
    print(f"  Accuracy: {metadata.performance_metrics['accuracy']:.2f}")
    print(f"  Precision: {metadata.performance_metrics['precision']:.2f}")
    print(f"  Recall: {metadata.performance_metrics['recall']:.2f}")
    print(f"  F1 Score: {metadata.performance_metrics['f1_score']:.2f}")


def report(metric: str) -> None:
    df = report_benchmark(metric)
    print(df)


if __name__ == "__main__":
    main()
